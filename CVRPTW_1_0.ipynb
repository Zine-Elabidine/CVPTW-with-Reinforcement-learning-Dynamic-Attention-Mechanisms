{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPvJQjdGAtkvH5P2QodM+JJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zine-Elabidine/CVPTW-with-Reinforcement-learning-Dynamic-Attention-Mechanisms/blob/main/CVRPTW_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "95Ujlurbl5CJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VRPTW Agent"
      ],
      "metadata": {
        "id": "pNI4f1KIUcgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class AgentVRP():\n",
        "\n",
        "    VEHICLE_CAPACITY = 1.0\n",
        "\n",
        "    def __init__(self, input):\n",
        "\n",
        "        depot = input[0]\n",
        "        loc = input[1]\n",
        "\n",
        "        self.batch_size, self.n_loc, _ = loc.shape  # (batch_size, n_nodes, 2)\n",
        "\n",
        "        # Coordinates of depot + other nodes\n",
        "        self.coords = tf.concat((depot[:, None, :], loc), -2)\n",
        "        self.demand = tf.cast(input[2], tf.float32)\n",
        "        self.time_windows = tf.cast(input[3], tf.float32)  # Time windows data\n",
        "\n",
        "        # Indices of graphs in batch\n",
        "        self.ids = tf.range(self.batch_size, dtype=tf.int64)[:, None]\n",
        "\n",
        "        # State\n",
        "        self.prev_a = tf.zeros((self.batch_size, 1), dtype=tf.float32)\n",
        "        self.from_depot = self.prev_a == 0\n",
        "        self.used_capacity = tf.zeros((self.batch_size, 1), dtype=tf.float32)\n",
        "\n",
        "        # Nodes that have been visited will be marked with 1\n",
        "        self.visited = tf.zeros((self.batch_size, 1, self.n_loc + 1), dtype=tf.uint8)\n",
        "\n",
        "        # Step counter\n",
        "        self.i = tf.zeros(1, dtype=tf.int64)\n",
        "\n",
        "        # Constant tensors for scatter update (in step method)\n",
        "        self.step_updates = tf.ones((self.batch_size, 1), dtype=tf.uint8)  # (batch_size, 1)\n",
        "        self.scatter_zeros = tf.zeros((self.batch_size, 1), dtype=tf.int64)  # (batch_size, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def outer_pr(a, b):\n",
        "        \"\"\"Outer product of matrices\n",
        "        \"\"\"\n",
        "        return tf.einsum('ki,kj->kij', a, b)\n",
        "\n",
        "    def get_att_mask(self):\n",
        "        \"\"\" Mask (batch_size, n_nodes, n_nodes) for attention encoder.\n",
        "            We mask already visited nodes except depot\n",
        "        \"\"\"\n",
        "\n",
        "        # We dont want to mask depot\n",
        "        att_mask = tf.squeeze(tf.cast(self.visited, tf.float32), axis=-2)[:, 1:]  # [batch_size, 1, n_nodes] --> [batch_size, n_nodes-1]\n",
        "\n",
        "        # Number of nodes in new instance after masking\n",
        "        cur_num_nodes = self.n_loc + 1 - tf.reshape(tf.reduce_sum(att_mask, -1), (-1,1))  # [batch_size, 1]\n",
        "\n",
        "        att_mask = tf.concat((tf.zeros(shape=(att_mask.shape[0],1),dtype=tf.float32),att_mask), axis=-1)\n",
        "\n",
        "        ones_mask = tf.ones_like(att_mask)\n",
        "\n",
        "        # Create square attention mask from row-like mask\n",
        "        att_mask = AgentVRP.outer_pr(att_mask, ones_mask) \\\n",
        "                            + AgentVRP.outer_pr(ones_mask, att_mask)\\\n",
        "                            - AgentVRP.outer_pr(att_mask, att_mask)\n",
        "\n",
        "        return tf.cast(att_mask, dtype=tf.bool), cur_num_nodes\n",
        "\n",
        "    def all_finished(self):\n",
        "        \"\"\"Checks if all games are finished\n",
        "        \"\"\"\n",
        "        return tf.reduce_all(tf.cast(self.visited, tf.bool))\n",
        "\n",
        "    def partial_finished(self):\n",
        "        \"\"\"Checks if partial solution for all graphs has been built, i.e. all agents came back to depot\n",
        "        \"\"\"\n",
        "        return tf.reduce_all(self.from_depot) and self.i != 0\n",
        "\n",
        "    def get_mask(self):\n",
        "        \"\"\" Returns a mask (batch_size, 1, n_nodes) with available actions.\n",
        "            Impossible nodes are masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # Exclude depot\n",
        "        visited_loc = self.visited[:, :, 1:]\n",
        "\n",
        "        # Mark nodes which exceed vehicle capacity\n",
        "        exceeds_cap = self.demand + self.used_capacity > self.VEHICLE_CAPACITY\n",
        "\n",
        "        # We mask nodes that are already visited or have too much demand\n",
        "        # Also for dynamical model we stop agent at depot when it arrives there (for partial solution)\n",
        "        mask_loc = tf.cast(visited_loc, tf.bool) | exceeds_cap[:, None, :] | ((self.i > 0) & self.from_depot[:, None, :])\n",
        "\n",
        "        # We can choose depot if 1) we are not in depot OR 2) all nodes are visited\n",
        "        mask_depot = self.from_depot & (tf.reduce_sum(tf.cast(mask_loc == False, tf.int32), axis=-1) > 0)\n",
        "\n",
        "        return tf.concat([mask_depot[:, :, None], mask_loc], axis=-1)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # Update current state\n",
        "        selected = action[:, None]\n",
        "\n",
        "        self.prev_a = selected\n",
        "        self.from_depot = self.prev_a == 0\n",
        "\n",
        "        # We have to shift indices by 1 since demand doesn't include depot\n",
        "        # 0-index in demand corresponds to the FIRST node\n",
        "        selected_demand = tf.gather_nd(self.demand,\n",
        "                                       tf.concat([self.ids, tf.clip_by_value(self.prev_a - 1, 0, self.n_loc - 1)], axis=1)\n",
        "                                       )[:, None]  # (batch_size, 1)\n",
        "\n",
        "        # We add current node capacity to used capacity and set it to zero if we return to the depot\n",
        "        self.used_capacity = (self.used_capacity + selected_demand) * (1.0 - tf.cast(self.from_depot, tf.float32))\n",
        "\n",
        "        # Update visited nodes (set 1 to visited nodes)\n",
        "        idx = tf.cast(tf.concat((self.ids, self.scatter_zeros, self.prev_a), axis=-1), tf.int32)[:, None, :]  # (batch_size, 1, 3)\n",
        "        self.visited = tf.tensor_scatter_nd_update(self.visited, idx, self.step_updates)  # (batch_size, 1, n_nodes)\n",
        "\n",
        "        self.i = self.i + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_costs(dataset, pi):\n",
        "\n",
        "        # Place nodes with coordinates in order of decoder tour\n",
        "        loc_with_depot = tf.concat([dataset[0][:, None, :], dataset[1]], axis=1)  # (batch_size, n_nodes, 2)\n",
        "        d = tf.gather(loc_with_depot, tf.cast(pi, tf.int32), batch_dims=1)\n",
        "\n",
        "        # Calculation of total distance\n",
        "        # Note: first element of pi is not depot, but the first selected node in the path\n",
        "        return (tf.reduce_sum(tf.norm(d[:, 1:] - d[:, :-1], ord='euclidean', axis=2), axis=1)# Changed from ord=2 to euclidean\n",
        "                + tf.norm(d[:, 0] - dataset[0], ord='euclidean', axis=1) # Distance from depot to first selected node\n",
        "                + tf.norm(d[:, -1] - dataset[0], ord='euclidean', axis=1))  # Distance from last selected node (!=0 for graph with longest path) to depot"
      ],
      "metadata": {
        "id": "Y9Ne7GLlUZ2m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers"
      ],
      "metadata": {
        "id": "Csqq6h35UoW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\" Attention Layer - multi-head scaled dot product attention (for encoder and decoder)\n",
        "        Args:\n",
        "            num_heads: number of attention heads which will be computed in parallel\n",
        "            d_model: embedding size of output features\n",
        "        Call arguments:\n",
        "            q: query, shape (..., seq_len_q, depth_q)\n",
        "            k: key, shape == (..., seq_len_k, depth_k)\n",
        "            v: value, shape == (..., seq_len_v, depth_v)\n",
        "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k) or None.\n",
        "            Since we use scaled-product attention, we assume seq_len_k = seq_len_v\n",
        "        Returns:\n",
        "              attention outputs of shape (batch_size, seq_len_q, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_depth = self.d_model // self.n_heads\n",
        "\n",
        "        if self.d_model % self.n_heads != 0:\n",
        "            raise ValueError(\"number of heads must divide d_model\")\n",
        "\n",
        "        # define weight matrices\n",
        "        self.wq = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_q, d_model)\n",
        "        self.wk = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_k, d_model)\n",
        "        self.wv = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_v, d_model)\n",
        "\n",
        "        self.w_out = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_model, d_model)\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\"Function for computing attention on several heads simultaneously\n",
        "        Splits last dimension of a tensor into (num_heads, head_depth).\n",
        "        Then we transpose it as (batch_size, num_heads, ..., head_depth) so that we can use broadcast\n",
        "        \"\"\"\n",
        "        tensor = tf.reshape(tensor, (batch_size, -1, self.n_heads, self.head_depth))\n",
        "        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # treats first parameter q as input, and  k, v as parameters, so input_shape=q.shape\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        # shape of q: (batch_size, seq_len_q, d_q)\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # compute Q = q * w_q, ...\n",
        "        Q = self.wq(q)  # (batch_size, seq_len_q, d_q) x (d_q, d_model) --> (batch_size, seq_len_q, d_model)\n",
        "        K = self.wk(k)  # ... --> (batch_size, seq_len_k, d_model)\n",
        "        V = self.wv(v)  # ... --> (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        # split heads: d_model = num_heads * head_depth + reshape\n",
        "        Q = self.split_heads(Q, batch_size)  # (batch_size, num_heads, seq_len_q, head_depth)\n",
        "        K = self.split_heads(K, batch_size)  # (batch_size, num_heads, seq_len_k, head_depth)\n",
        "        V = self.split_heads(V, batch_size)  # (batch_size, num_heads, seq_len_v, head_depth)\n",
        "\n",
        "        # similarity between context vector Q and key K // self-similarity in case of self-attention\n",
        "        compatibility = tf.matmul(Q, K, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "                                                           # seq_len_q = n_nodes for encoder self-attention\n",
        "                                                           # seq_len_q = 1 for decoder context-vector attention\n",
        "                                                           # seq_len_k = n_nodes for both encoder & decoder\n",
        "        # rescaling\n",
        "        dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "        compatibility = compatibility / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            # we need to reshape mask:\n",
        "            # (batch_size, seq_len_q, seq_len_k) --> (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            # so that we will be able to do a broadcast:\n",
        "            # (batch_size, num_heads, seq_len_q, seq_len_k) + (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            mask = mask[:, tf.newaxis, :, :]\n",
        "\n",
        "            # we use tf.where since 0*-np.inf returns nan, but not -np.inf\n",
        "            # compatibility = tf.where(\n",
        "            #                     tf.broadcast_to(mask, compatibility.shape), tf.ones_like(compatibility) * (-np.inf),\n",
        "            #                     compatibility\n",
        "            #                      )\n",
        "\n",
        "            compatibility = tf.where(mask,\n",
        "                                    tf.ones_like(compatibility) * (-np.inf),\n",
        "                                    compatibility)\n",
        "\n",
        "        compatibility = tf.nn.softmax(compatibility, axis=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        # Replace NaN by zeros (tf.nn.softmax returns NaNs for masked rows)\n",
        "        compatibility = tf.where(tf.math.is_nan(compatibility), tf.zeros_like(compatibility), compatibility)\n",
        "\n",
        "        # seq_len_k = seq_len_v\n",
        "        attention = tf.matmul(compatibility, V)  # (batch_size, num_heads, seq_len_q, head_depth)\n",
        "\n",
        "        # transpose back to (batch_size, seq_len_q, num_heads, head_depth)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # concatenate heads (last 2 dimensions)\n",
        "        attention = tf.reshape(attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # project output to the same dimension\n",
        "        # this is equiv. to sum in the article (project heads with W_o and sum), beacuse of block-matrix multiplication\n",
        "        #e.g. https://math.stackexchange.com/questions/2961550/matrix-block-multiplication-definition-properties-and-applications\n",
        "\n",
        "        output = self.w_out(attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "HvTqFRFctS3g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Encoder"
      ],
      "metadata": {
        "id": "i-tEy4zpV43c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Feed-Forward Sublayer: fully-connected Feed-Forward network,\n",
        "    built based on MHA vectors from MultiHeadAttention layer with skip-connections\n",
        "        Args:\n",
        "            num_heads: number of attention heads in MHA layers.\n",
        "            input_dim: embedding size that will be used as d_model in MHA layers.\n",
        "            feed_forward_hidden: number of neuron units in each FF layer.\n",
        "        Call arguments:\n",
        "            x: batch of shape (batch_size, n_nodes, node_embedding_size).\n",
        "            mask: mask for MHA layer\n",
        "        Returns:\n",
        "               outputs of shape (batch_size, n_nodes, input_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, feed_forward_hidden=512, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(n_heads=num_heads, d_model=input_dim, name='MHA')\n",
        "        self.ff1 = tf.keras.layers.Dense(feed_forward_hidden, name='ff1')\n",
        "        self.ff2 = tf.keras.layers.Dense(input_dim, name='ff2')\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        mha_out = self.mha(x, x, x, mask)\n",
        "        sc1_out = tf.keras.layers.Add()([x, mha_out])\n",
        "        tanh1_out = tf.keras.activations.tanh(sc1_out)\n",
        "\n",
        "        ff1_out = self.ff1(tanh1_out)\n",
        "        relu1_out = tf.keras.activations.relu(ff1_out)\n",
        "        ff2_out = self.ff2(relu1_out)\n",
        "        sc2_out = tf.keras.layers.Add()([tanh1_out, ff2_out])\n",
        "        tanh2_out = tf.keras.activations.tanh(sc2_out)\n",
        "\n",
        "        return tanh2_out\n",
        "\n",
        "class GraphAttentionEncoder(tf.keras.layers.Layer):\n",
        "    \"\"\"Graph Encoder, which uses MultiHeadAttentionLayer sublayer.\n",
        "        Args:\n",
        "            input_dim: embedding size that will be used as d_model in MHA layers.\n",
        "            num_heads: number of attention heads in MHA layers.\n",
        "            num_layers: number of attention layers that will be used in encoder.\n",
        "            feed_forward_hidden: number of neuron units in each FF layer.\n",
        "        Call arguments:\n",
        "            x: tuples of 3 tensors:  (batch_size, 2), (batch_size, n_nodes-1, 2), (batch_size, n_nodes-1)\n",
        "            First tensor contains coordinates for depot, second one is for coordinates of other nodes,\n",
        "            Last tensor is for normalized demands for nodes except depot\n",
        "            mask: mask for MHA layer\n",
        "        Returns:\n",
        "               Embedding for all nodes + mean embedding for graph.\n",
        "               Tuples ((batch_size, n_nodes, input_dim), (batch_size, input_dim))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, num_layers, feed_forward_hidden=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.feed_forward_hidden = feed_forward_hidden\n",
        "\n",
        "        # initial embeddings (batch_size, n_nodes-1, 2) --> (batch-size, input_dim), separate for depot and other nodes\n",
        "        self.init_embed_depot = tf.keras.layers.Dense(self.input_dim, name='init_embed_depot')  # nn.Linear(2, embedding_dim)\n",
        "        self.init_embed = tf.keras.layers.Dense(self.input_dim, name='init_embed')\n",
        "\n",
        "        self.mha_layers = [MultiHeadAttentionLayer(self.input_dim, self.num_heads, self.feed_forward_hidden)\n",
        "                            for _ in range(self.num_layers)]\n",
        "\n",
        "    def call(self, x, mask=None, cur_num_nodes=None):\n",
        "\n",
        "        x = tf.concat((self.init_embed_depot(x[0])[:, None, :],  # (batch_size, 2) --> (batch_size, 1, 2)\n",
        "                       self.init_embed(tf.concat((x[1], x[2][:, :, None]), axis=-1))  # (batch_size, n_nodes-1, 2) + (batch_size, n_nodes-1)\n",
        "                       ), axis=1)  # (batch_size, n_nodes, input_dim)\n",
        "\n",
        "        # stack attention layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.mha_layers[i](x, mask)\n",
        "\n",
        "        if mask is not None:\n",
        "            output = (x, tf.reduce_sum(x, axis=1) / cur_num_nodes)\n",
        "        else:\n",
        "            output = (x, tf.reduce_mean(x, axis=1))\n",
        "\n",
        "        return output # (embeds of nodes, avg graph embed)=((batch_size, n_nodes, input), (batch_size, input_dim))"
      ],
      "metadata": {
        "id": "a23VQpNVx4oQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention dynamic"
      ],
      "metadata": {
        "id": "l5BicuzgV8ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#from attention_graph_encoder import GraphAttentionEncoder\n",
        "#from enviroment import AgentVRP\n",
        "\n",
        "\n",
        "def set_decode_type(model, decode_type):\n",
        "    model.set_decode_type(decode_type)\n",
        "\n",
        "class AttentionDynamicModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim,\n",
        "                 n_encode_layers=3,\n",
        "                 n_heads=8,\n",
        "                 tanh_clipping=10.\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # attributes for MHA\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.decode_type = None\n",
        "\n",
        "        # attributes for VRP problem\n",
        "        self.problem = AgentVRP\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Encoder part\n",
        "        self.embedder = GraphAttentionEncoder(input_dim=self.embedding_dim,\n",
        "                                              num_heads=self.n_heads,\n",
        "                                              num_layers=self.n_encode_layers\n",
        "                                              )\n",
        "\n",
        "        # Decoder part\n",
        "\n",
        "        self.output_dim = self.embedding_dim\n",
        "        self.num_heads = n_heads\n",
        "\n",
        "        self.head_depth = self.output_dim // self.num_heads\n",
        "        self.dk_mha_decoder = tf.cast(self.head_depth, tf.float32)  # for decoding in mha_decoder\n",
        "        self.dk_get_loc_p = tf.cast(self.output_dim, tf.float32)  # for decoding in mha_decoder\n",
        "\n",
        "        if self.output_dim % self.num_heads != 0:\n",
        "            raise ValueError(\"number of heads must divide d_model=output_dim\")\n",
        "\n",
        "        self.tanh_clipping = tanh_clipping\n",
        "\n",
        "        # we split projection matrix Wq into 2 matrices: Wq*[h_c, h_N, D] = Wq_context*h_c + Wq_step_context[h_N, D]\n",
        "        self.wq_context = tf.keras.layers.Dense(self.output_dim, use_bias=False,\n",
        "                                                name='wq_context')  # (d_q_context, output_dim)\n",
        "        self.wq_step_context = tf.keras.layers.Dense(self.output_dim, use_bias=False,\n",
        "                                                     name='wq_step_context')  # (d_q_step_context, output_dim)\n",
        "\n",
        "        # we need two Wk projections since there is MHA followed by 1-head attention - they have different keys K\n",
        "        self.wk = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='wk')  # (d_k, output_dim)\n",
        "        self.wk_tanh = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='wk_tanh')  # (d_k_tanh, output_dim)\n",
        "\n",
        "        # we dont need Wv projection for 1-head attention: only need attention weights as outputs\n",
        "        self.wv = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='wv')  # (d_v, output_dim)\n",
        "\n",
        "        # we dont need wq for 1-head tanh attention, since we can absorb it into w_out\n",
        "        self.w_out = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='w_out')  # (d_model, d_model)\n",
        "\n",
        "    def set_decode_type(self, decode_type):\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\"Function for computing attention on several heads simultaneously\n",
        "        Splits last dimension of a tensor into (num_heads, head_depth).\n",
        "        Then we transpose it as (batch_size, num_heads, ..., head_depth) so that we can use broadcast\n",
        "        \"\"\"\n",
        "        tensor = tf.reshape(tensor, (batch_size, -1, self.num_heads, self.head_depth))\n",
        "        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def _select_node(self, logits):\n",
        "        \"\"\"Select next node based on decoding type.\n",
        "        \"\"\"\n",
        "\n",
        "        # assert tf.reduce_all(logits == logits), \"Probs should not contain any nans\"\n",
        "\n",
        "        if self.decode_type == \"greedy\":\n",
        "            selected = tf.math.argmax(logits, axis=-1)  # (batch_size, 1)\n",
        "\n",
        "        elif self.decode_type == \"sampling\":\n",
        "            # logits has a shape of (batch_size, 1, n_nodes), we have to squeeze it\n",
        "            # to (batch_size, n_nodes) since tf.random.categorical requires matrix\n",
        "            selected = tf.random.categorical(logits[:, 0, :], 1)  # (bach_size,1)\n",
        "        else:\n",
        "            assert False, \"Unknown decode type\"\n",
        "\n",
        "        return tf.squeeze(selected, axis=-1)  # (bach_size,)\n",
        "\n",
        "    def get_step_context(self, state, embeddings):\n",
        "        \"\"\"Takes a state and graph embeddings,\n",
        "           Returns a part [h_N, D] of context vector [h_c, h_N, D],\n",
        "           that is related to RL Agent last step.\n",
        "        \"\"\"\n",
        "        # index of previous node\n",
        "        prev_node = state.prev_a  # (batch_size, 1)\n",
        "\n",
        "        # from embeddings=(batch_size, n_nodes, input_dim) select embeddings of previous nodes\n",
        "        cur_embedded_node = tf.gather(embeddings, tf.cast(prev_node, tf.int32), batch_dims=1)  # (batch_size, 1, input_dim)\n",
        "\n",
        "        # add remaining capacity\n",
        "        step_context = tf.concat([cur_embedded_node, self.problem.VEHICLE_CAPACITY - state.used_capacity[:, :, None]], axis=-1)\n",
        "\n",
        "        return step_context  # (batch_size, 1, input_dim + 1)\n",
        "\n",
        "    def decoder_mha(self, Q, K, V, mask=None):\n",
        "        \"\"\" Computes Multi-Head Attention part of decoder\n",
        "        Basically, its a part of MHA sublayer, but we cant construct a layer since Q changes in a decoding loop.\n",
        "        Args:\n",
        "            mask: a mask for visited nodes,\n",
        "                has shape (batch_size, seq_len_q, seq_len_k), seq_len_q = 1 for context vector attention in decoder\n",
        "            Q: query (context vector for decoder)\n",
        "                    has shape (..., seq_len_q, head_depth) with seq_len_q = 1 for context_vector attention in decoder\n",
        "            K, V: key, value (projections of nodes embeddings)\n",
        "                have shape (..., seq_len_k, head_depth), (..., seq_len_v, head_depth),\n",
        "                                                                with seq_len_k = seq_len_v = n_nodes for decoder\n",
        "        \"\"\"\n",
        "\n",
        "        compatibility = tf.matmul(Q, K, transpose_b=True)/tf.math.sqrt(self.dk_mha_decoder)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        if mask is not None:\n",
        "\n",
        "            # we need to reshape mask:\n",
        "            # (batch_size, seq_len_q, seq_len_k) --> (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            # so that we will be able to do a broadcast:\n",
        "            # (batch_size, num_heads, seq_len_q, seq_len_k) + (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            mask = mask[:, tf.newaxis, :, :]\n",
        "\n",
        "            # we use tf.where since 0*-np.inf returns nan, but not -np.inf\n",
        "            # compatibility = tf.where(\n",
        "            #                     tf.broadcast_to(mask, compatibility.shape), tf.ones_like(compatibility) * (-np.inf),\n",
        "            #                     compatibility\n",
        "            #                      )\n",
        "\n",
        "            compatibility = tf.where(mask,\n",
        "                                     tf.ones_like(compatibility) * (-np.inf),\n",
        "                                     compatibility\n",
        "                                     )\n",
        "\n",
        "\n",
        "        compatibility = tf.nn.softmax(compatibility, axis=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        attention = tf.matmul(compatibility, V)  # (batch_size, num_heads, seq_len_q, head_depth)\n",
        "\n",
        "        # transpose back to (batch_size, seq_len_q, num_heads, depth)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # concatenate heads (last 2 dimensions)\n",
        "        attention = tf.reshape(attention, (self.batch_size, -1, self.output_dim))  # (batch_size, seq_len_q, output_dim)\n",
        "\n",
        "        output = self.w_out(attention)  # (batch_size, seq_len_q, output_dim), seq_len_q = 1 for context att in decoder\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_log_p(self, Q, K, mask=None):\n",
        "        \"\"\"Single-Head attention sublayer in decoder,\n",
        "        computes log-probabilities for node selection.\n",
        "        Args:\n",
        "            mask: mask for nodes\n",
        "            Q: query (output of mha layer)\n",
        "                    has shape (batch_size, seq_len_q, output_dim), seq_len_q = 1 for context attention in decoder\n",
        "            K: key (projection of node embeddings)\n",
        "                    has shape  (batch_size, seq_len_k, output_dim), seq_len_k = n_nodes for decoder\n",
        "        \"\"\"\n",
        "\n",
        "        compatibility = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(self.dk_get_loc_p)\n",
        "        compatibility = tf.math.tanh(compatibility) * self.tanh_clipping\n",
        "\n",
        "        if mask is not None:\n",
        "\n",
        "            # we dont need to reshape mask like we did in multi-head version:\n",
        "            # (batch_size, seq_len_q, seq_len_k) --> (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "            # since we dont have multiple heads\n",
        "\n",
        "            # compatibility = tf.where(\n",
        "            #                     tf.broadcast_to(mask, compatibility.shape), tf.ones_like(compatibility) * (-np.inf),\n",
        "            #                     compatibility\n",
        "            #                      )\n",
        "\n",
        "            compatibility = tf.where(mask,\n",
        "                                     tf.ones_like(compatibility) * (-np.inf),\n",
        "                                     compatibility\n",
        "                                     )\n",
        "\n",
        "        log_p = tf.nn.log_softmax(compatibility, axis=-1)  # (batch_size, seq_len_q, seq_len_k)\n",
        "\n",
        "        return log_p\n",
        "\n",
        "    def get_log_likelihood(self, _log_p, a):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = tf.gather_nd(_log_p, tf.cast(tf.expand_dims(a, axis=-1), tf.int32), batch_dims=2)\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return tf.reduce_sum(log_p,1)\n",
        "\n",
        "    def get_projections(self, embeddings, context_vectors):\n",
        "\n",
        "        # we compute some projections (common for each policy step) before decoding loop for efficiency\n",
        "        K = self.wk(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        K_tanh = self.wk_tanh(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        V = self.wv(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        Q_context = self.wq_context(context_vectors[:, tf.newaxis, :])  # (batch_size, 1, output_dim)\n",
        "\n",
        "        # we dont need to split K_tanh since there is only 1 head; Q will be split in decoding loop\n",
        "        K = self.split_heads(K, self.batch_size)  # (batch_size, num_heads, n_nodes, head_depth)\n",
        "        V = self.split_heads(V, self.batch_size)  # (batch_size, num_heads, n_nodes, head_depth)\n",
        "\n",
        "        return K_tanh, Q_context, K, V\n",
        "\n",
        "    def call(self, inputs, return_pi=False):\n",
        "\n",
        "        embeddings, mean_graph_emb = self.embedder(inputs)\n",
        "\n",
        "        self.batch_size = tf.shape(embeddings)[0]\n",
        "\n",
        "        outputs = []\n",
        "        sequences = []\n",
        "\n",
        "        state = self.problem(inputs)\n",
        "\n",
        "        K_tanh, Q_context, K, V = self.get_projections(embeddings, mean_graph_emb)\n",
        "\n",
        "        # Perform decoding steps\n",
        "        i = 0\n",
        "        inner_i = 0\n",
        "\n",
        "        while not state.all_finished():\n",
        "\n",
        "            if i > 0:\n",
        "                state.i = tf.zeros(1, dtype=tf.int64)\n",
        "                att_mask, cur_num_nodes = state.get_att_mask()\n",
        "                embeddings, context_vectors = self.embedder(inputs, att_mask, cur_num_nodes)\n",
        "                K_tanh, Q_context, K, V = self.get_projections(embeddings, context_vectors)\n",
        "\n",
        "            inner_i = 0\n",
        "            while not state.partial_finished():\n",
        "\n",
        "                step_context = self.get_step_context(state, embeddings)  # (batch_size, 1), (batch_size, 1, input_dim + 1)\n",
        "                Q_step_context = self.wq_step_context(step_context)  # (batch_size, 1, output_dim)\n",
        "                Q = Q_context + Q_step_context\n",
        "\n",
        "                # split heads for Q\n",
        "                Q = self.split_heads(Q, self.batch_size)  # (batch_size, num_heads, 1, head_depth)\n",
        "\n",
        "                # get current mask\n",
        "                mask = state.get_mask()  # (batch_size, 1, n_nodes) with True/False indicating where agent can go\n",
        "\n",
        "                # compute MHA decoder vectors for current mask\n",
        "                mha = self.decoder_mha(Q, K, V, mask)  # (batch_size, 1, output_dim)\n",
        "\n",
        "                # compute probabilities\n",
        "                log_p = self.get_log_p(mha, K_tanh, mask)  # (batch_size, 1, n_nodes)\n",
        "\n",
        "                # next step is to select node\n",
        "                selected = self._select_node(log_p)\n",
        "\n",
        "                state.step(selected)\n",
        "\n",
        "                outputs.append(log_p[:, 0, :])\n",
        "                sequences.append(selected)\n",
        "\n",
        "                inner_i += 1\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        _log_p, pi = tf.stack(outputs, 1), tf.cast(tf.stack(sequences, 1), tf.float32)\n",
        "\n",
        "        cost = self.problem.get_costs(inputs, pi)\n",
        "\n",
        "        ll = self.get_log_likelihood(_log_p, pi)\n",
        "\n",
        "        if return_pi:\n",
        "            return cost, ll, pi\n",
        "\n",
        "        return cost, ll"
      ],
      "metadata": {
        "id": "My1GeN_YV8WH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "oXvXSB70WOTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "def create_data_on_disk(graph_size, num_samples, is_save=True, filename=None, is_return=False, seed=1234):\n",
        "    \"\"\"Generate validation dataset (with SEED) and save \"\"\"\n",
        "    CAPACITIES = {6: 10., 8: 10.}\n",
        "    TIME_WINDOW_RANGE = 72  # Assuming time windows are within a 24-hour range\n",
        "\n",
        "    depo = tf.random.uniform(minval=0, maxval=1, shape=(num_samples, 2), seed=seed)\n",
        "    graphs = tf.random.uniform(minval=0, maxval=1, shape=(num_samples, graph_size, 2), seed=seed)\n",
        "    demand = tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(num_samples, graph_size), dtype=tf.int32, seed=seed), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)\n",
        "\n",
        "    # Generate time windows\n",
        "    start_times = tf.random.uniform(minval=0, maxval=TIME_WINDOW_RANGE-24, shape=(num_samples, graph_size), dtype=tf.int32, seed=seed)\n",
        "    end_times = start_times + tf.random.uniform(minval=1, maxval=24, shape=(num_samples, graph_size), dtype=tf.int32, seed=seed)  # Assuming time window duration is between 1 and 6 hours\n",
        "    time_windows = tf.stack([start_times, end_times], axis=-1)\n",
        "\n",
        "    if is_save:\n",
        "        save_to_pickle(f'Validation_dataset_{filename}.pkl', (depo, graphs, demand, time_windows))\n",
        "\n",
        "    if is_return:\n",
        "        return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand), list(time_windows)))\n",
        "\n",
        "\n",
        "def save_to_pickle(filename, item):\n",
        "    \"\"\"Save to pickle\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as handle:\n",
        "        pickle.dump(item, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def read_from_pickle(path, return_tf_data_set=True, num_samples=None):\n",
        "    \"\"\"Read dataset from file (pickle)\n",
        "    \"\"\"\n",
        "\n",
        "    objects = []\n",
        "    with (open(path, \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    objects = objects[0]\n",
        "    if return_tf_data_set:\n",
        "        depo, graphs, demand,time_windows = objects\n",
        "        if num_samples is not None:\n",
        "            return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand),list(time_windows))).take(num_samples)\n",
        "        else:\n",
        "            return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand),list(time_windows)))\n",
        "    else:\n",
        "        return objects\n",
        "\n",
        "\n",
        "def generate_data_onfly(num_samples=10000, graph_size=6):\n",
        "    \"\"\"Generate temp dataset in memory\n",
        "    \"\"\"\n",
        "\n",
        "    CAPACITIES = {\n",
        "        6: 10.,\n",
        "        8: 10.\n",
        "\n",
        "    }\n",
        "    TIME_WINDOW_RANGE = 72  # Assuming time windows are within a 24-hour range\n",
        "\n",
        "    depo = tf.random.uniform(minval=0, maxval=1, shape=(num_samples, 2))\n",
        "    graphs = tf.random.uniform(minval=0, maxval=1, shape=(num_samples, graph_size, 2))\n",
        "    demand = tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(num_samples, graph_size), dtype=tf.int32), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)\n",
        "\n",
        "    # Generate time windows\n",
        "    start_times = tf.random.uniform(minval=0, maxval=TIME_WINDOW_RANGE-24, shape=(num_samples, graph_size), dtype=tf.int32)\n",
        "    end_times = start_times + tf.random.uniform(minval=1, maxval=24, shape=(num_samples, graph_size), dtype=tf.int32)  # Assuming time window duration is between 1 and 6 hours\n",
        "    time_windows = tf.stack([start_times, end_times], axis=-1)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand), list(time_windows)))\n",
        "\n",
        "\n",
        "def get_results(train_loss_results, train_cost_results, val_cost, save_results=True, filename=None, plots=True):\n",
        "\n",
        "    epochs_num = len(train_loss_results)\n",
        "\n",
        "    df_train = pd.DataFrame(data={'epochs': list(range(epochs_num)),\n",
        "                                  'loss': train_loss_results,\n",
        "                                  'cost': train_cost_results,\n",
        "                                  })\n",
        "    df_test = pd.DataFrame(data={'epochs': list(range(epochs_num)),\n",
        "                                 'val_сost': val_cost})\n",
        "    if save_results:\n",
        "        df_train.to_excel('train_results_{}.xlsx'.format(filename), index=False)\n",
        "        df_test.to_excel('test_results_{}.xlsx'.format(filename), index=False)\n",
        "\n",
        "    if plots:\n",
        "        plt.figure(figsize=(15, 9))\n",
        "        ax = sns.lineplot(x='epochs', y='loss', data=df_train, color='salmon', label='train loss')\n",
        "        ax2 = ax.twinx()\n",
        "        sns.lineplot(x='epochs', y='cost', data=df_train, color='cornflowerblue', label='train cost', ax=ax2)\n",
        "        sns.lineplot(x='epochs', y='val_сost', data=df_test, palette='darkblue', label='val cost').set(ylabel='cost')\n",
        "\n",
        "        ax.legend(loc=(0.75, 0.90), ncol=1)\n",
        "        ax2.legend(loc=(0.75, 0.95), ncol=2)\n",
        "        ax.grid(axis='x')\n",
        "        ax2.grid(True)\n",
        "        plt.savefig('learning_curve_plot_{}.jpg'.format(filename))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def get_journey(batch, pi,GRAPH_SIZE, ind_in_batch=0):\n",
        "    \"\"\"Plots journey of agent\n",
        "    Args:\n",
        "        batch: dataset of graphs\n",
        "        pi: paths of agent obtained from model\n",
        "        ind_in_batch: index of graph in batch to be plotted\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove extra zeros\n",
        "    pi_ = get_clean_path(pi[ind_in_batch].numpy())\n",
        "    CAPACITIES = {\n",
        "        6: 10.,\n",
        "        8: 10.\n",
        "                  }\n",
        "\n",
        "    # Unpack variables\n",
        "    depo_coord = batch[0][ind_in_batch].numpy()\n",
        "    points_coords = batch[1][ind_in_batch].numpy()\n",
        "    demands = batch[2][ind_in_batch].numpy()*CAPACITIES[GRAPH_SIZE]\n",
        "    node_labels = ['(' + str(x[0]) + ', ' + x[1] + ')' for x in enumerate(demands.round(2).astype(str))]\n",
        "\n",
        "    # Concatenate depot and points\n",
        "    full_coords = np.concatenate((depo_coord.reshape(1, 2), points_coords))\n",
        "\n",
        "    # Get list with agent loops in path\n",
        "    list_of_paths = []\n",
        "    cur_path = []\n",
        "    for idx, node in enumerate(pi_):\n",
        "\n",
        "        cur_path.append(node)\n",
        "\n",
        "        if idx != 0 and node == 0:\n",
        "            if cur_path[0] != 0:\n",
        "                cur_path.insert(0, 0)\n",
        "            list_of_paths.append(cur_path)\n",
        "            cur_path = []\n",
        "\n",
        "    Total_distance=0\n",
        "    list_of_path_traces = []\n",
        "    for path_counter, path in enumerate(list_of_paths):\n",
        "        coords = full_coords[[int(x) for x in path]]\n",
        "\n",
        "        # Calculate length of each agent loop\n",
        "        lengths = np.sqrt(np.sum(np.diff(coords, axis=0) ** 2, axis=1))\n",
        "        total_length = np.sum(lengths)\n",
        "        Total_distance+=total_length\n",
        "\n",
        "        list_of_path_traces.append(go.Scatter(x=coords[:, 0],\n",
        "                                              y=coords[:, 1],\n",
        "                                              mode=\"markers+lines\",\n",
        "                                              name=f\"path_{path_counter+1}, length={total_length:.2f}\",\n",
        "                                              opacity=1.0))\n",
        "    Total_distance\n",
        "    trace_points = go.Scatter(x=points_coords[:, 0],\n",
        "                              y=points_coords[:, 1],\n",
        "                              mode='markers+text',\n",
        "                              name='destinations',\n",
        "                              text=node_labels,\n",
        "                              textposition='top center',\n",
        "                              marker=dict(size=7),\n",
        "                              opacity=1.0\n",
        "                              )\n",
        "\n",
        "    trace_depo = go.Scatter(x=[depo_coord[0]],\n",
        "                            y=[depo_coord[1]],\n",
        "                            text=['1.0'], textposition='bottom center',\n",
        "                            mode='markers+text',\n",
        "                            marker=dict(size=15),\n",
        "                            name='depot'\n",
        "                            )\n",
        "\n",
        "    layout = go.Layout(title='<b>{}_Customers_ML_Model_Total_Distance_{}</b>'.format(GRAPH_SIZE,round(Total_distance,2)),\n",
        "                       xaxis=dict(title='X coordinate'),\n",
        "                       yaxis=dict(title='Y coordinate'),\n",
        "                       showlegend=True,\n",
        "                       width=1000,\n",
        "                       height=1000,\n",
        "                       template=\"plotly_white\"\n",
        "                       )\n",
        "\n",
        "    data = [trace_points, trace_depo] + list_of_path_traces\n",
        "    print('Current path: ', pi_)\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def get_cur_time():\n",
        "    \"\"\"Returns local time as string\n",
        "    \"\"\"\n",
        "    ts = time.time()\n",
        "    return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "def get_clean_path(arr):\n",
        "    \"\"\"Returns extra zeros from path.\n",
        "       Dynamical model generates duplicated zeros for several graphs when obtaining partial solutions.\n",
        "    \"\"\"\n",
        "\n",
        "    p1, p2 = 0, 1\n",
        "    output = []\n",
        "\n",
        "    while p2 < len(arr):\n",
        "\n",
        "        if arr[p1] != arr[p2]:\n",
        "            output.append(arr[p1])\n",
        "            if p2 == len(arr) - 1:\n",
        "                output.append(arr[p2])\n",
        "\n",
        "        p1 += 1\n",
        "        p2 += 1\n",
        "\n",
        "    if output[0] != 0:\n",
        "        output.insert(0, 0.0)\n",
        "    if output[-1] != 0:\n",
        "        output.append(0.0)\n",
        "\n",
        "    return output\n",
        "\n",
        "def get_journey_savings(tour,routes, GRAPH_SIZE):\n",
        "  import plotly.graph_objects as go\n",
        "  import numpy as np\n",
        "  depot_cord=tour[0].numpy().squeeze()\n",
        "  location_values=tour[1].numpy().squeeze()\n",
        "  CAPACITIES = {\n",
        "        6: 10.,\n",
        "        8: 10.\n",
        "\n",
        "                  }\n",
        "  demand_values=tour[2][0].numpy()*CAPACITIES[GRAPH_SIZE]\n",
        "  all_coords = np.concatenate((depot_cord.reshape(1, 2), location_values))\n",
        "  node_labels = ['(' + str(x[0]) + ', ' + x[1] + ')' for x in enumerate(demand_values.round(2).astype(str))]\n",
        "  list_of_path_traces=[]\n",
        "  total_length=0\n",
        "\n",
        "  for k,v in routes.items():\n",
        "    # Calculate length of each agent loop\n",
        "    lengths = np.sum(np.sqrt(np.sum(np.diff(v, axis=0) ** 2, axis=1)))\n",
        "    total_length+=lengths\n",
        "    list_of_path_traces.append(go.Scatter(x=np.array(v)[:, 0],\n",
        "                                           y=np.array(v)[:, 1],\n",
        "                                           mode=\"markers+lines\",\n",
        "                                           name=f\"path_{k+1}, length={lengths:.2f}\",\n",
        "                                           opacity=1.0))\n",
        "\n",
        "  trace_points = go.Scatter(x=location_values[:, 0],\n",
        "                              y=location_values[:, 1],\n",
        "                              mode='markers+text',\n",
        "                              name='destinations',\n",
        "                              text=node_labels,\n",
        "                              textposition='top center',\n",
        "                              marker=dict(size=7),\n",
        "                              opacity=1.0\n",
        "                              )\n",
        "\n",
        "  depo_point = go.Scatter(x=[depot_cord[0]],\n",
        "                            y=[depot_cord[1]],\n",
        "                            text=['1.0'], textposition='bottom center',\n",
        "                            mode='markers+text',\n",
        "                            marker=dict(size=15),\n",
        "                            name='depot'\n",
        "                            )\n",
        "  layout = go.Layout(title='<b>{}_Customers_Savings_Algorithm_Total_Distance_{}</b>'.format(GRAPH_SIZE,round(total_length,2)),\n",
        "                       xaxis=dict(title='X coordinate'),\n",
        "                       yaxis=dict(title='Y coordinate'),\n",
        "                       showlegend=True,\n",
        "                       width=1000,\n",
        "                       height=1000,\n",
        "                       template=\"plotly_white\"\n",
        "                       )\n",
        "  data = [trace_points, depo_point]+ list_of_path_traces\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "m64Mong_WOCZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils demo"
      ],
      "metadata": {
        "id": "T2l6WW8CWeVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def f_get_results_plot_seaborn(data, title, graph_size=20):\n",
        "    fig = plt.figure(figsize=(15, 9))\n",
        "    ax = fig.add_subplot()\n",
        "    ax.plot(data['epochs'], data['train_loss'], color='salmon', label='train loss')\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(data['epochs'], data['train_cost'],  color='cornflowerblue', label='train cost')\n",
        "    ax2.plot(data['epochs'], data['val_cost'], color='darkblue', label='val cost')\n",
        "\n",
        "    if graph_size == 20:\n",
        "        am_val = 6.4\n",
        "    else:\n",
        "        am_val = 10.98\n",
        "\n",
        "    plt.axhline(y=am_val, color='black', linestyle='--', linewidth=1.5, label='AM article best score')\n",
        "\n",
        "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)\n",
        "\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax2.set_ylabel('Cost')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.grid(False)\n",
        "    ax2.grid(False)\n",
        "    ax2.set_yticks(np.arange(min(data['val_cost'].min(), data['train_cost'].min())-0.2,\n",
        "                             max(data['val_cost'].max(), data['train_cost'].max())+0.1,\n",
        "                             0.1).round(2))\n",
        "    plt.title('Learning Curve: ' + title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def f_get_results_plot_plotly(data, title, graph_size=20):\n",
        "    # Create figure with secondary y-axis\n",
        "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    # Add traces\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=data['epochs'], y=data['train_loss'], name=\"train loss\", marker_color='salmon'),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=data['epochs'], y=data['train_cost'], name=\"train cost\", marker_color='cornflowerblue'),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=data['epochs'], y=data['val_cost'], name=\"val cost\", marker_color='darkblue'),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    # Add figure title\n",
        "    fig.update_layout(\n",
        "        title_text=\"Learning Curve: \" + title,\n",
        "        width=950,\n",
        "        height=650,\n",
        "        # plot_bgcolor='rgba(0,0,0,0)'\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # Set x-axis title\n",
        "    fig.update_xaxes(title_text=\"Number of epoch\")\n",
        "\n",
        "    # Set y-axes titles\n",
        "    fig.update_yaxes(title_text=\"<b>Loss\", secondary_y=False, showgrid=False, zeroline=False)\n",
        "    fig.update_yaxes(title_text=\"<b>Cost\", secondary_y=True, dtick=0.1)#, nticks=20)\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "1tiqYS7vWN-y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "BEJtYwVmWveC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from scipy.stats import ttest_rel\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "#from attention_dynamic_model import AttentionDynamicModel\n",
        "#from attention_dynamic_model import set_decode_type\n",
        "#from utils import generate_data_onfly\n",
        "\n",
        "\n",
        "def copy_of_tf_model(model, embedding_dim=128, graph_size=6):\n",
        "    \"\"\"Copy model weights to new model with time windows\"\"\"\n",
        "    # https://stackoverflow.com/questions/56841736/how-to-copy-a-network-in-tensorflow-2-0\n",
        "    CAPACITIES = {6: 10., 8: 10.}\n",
        "    TIME_WINDOW_RANGE = 72  # Assuming time windows are within a 24-hour range\n",
        "\n",
        "    depot = tf.random.uniform((2, 2), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    loc = tf.random.uniform((2, graph_size, 2), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    demand = tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(2, graph_size), dtype=tf.int32), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)\n",
        "\n",
        "    # Generate time windows\n",
        "    start_times = tf.random.uniform(minval=0, maxval=TIME_WINDOW_RANGE-24, shape=(2, graph_size), dtype=tf.int32)\n",
        "    end_times = start_times + tf.random.uniform(minval=1, maxval=24, shape=(2, graph_size), dtype=tf.int32)  # Assuming time window duration is between 1 and 6 hours\n",
        "    time_windows = tf.stack([start_times, end_times], axis=-1)\n",
        "\n",
        "    data_random = [depot, loc, demand, time_windows]\n",
        "\n",
        "    new_model = AttentionDynamicModel(embedding_dim)\n",
        "    set_decode_type(new_model, \"sampling\")\n",
        "    _, _ = new_model(data_random)\n",
        "\n",
        "    for a, b in zip(new_model.variables, model.variables):\n",
        "        a.assign(b)\n",
        "\n",
        "    return new_model\n",
        "def rollout(model, dataset, batch_size = 1000, disable_tqdm = False):\n",
        "    # Evaluate model in greedy mode\n",
        "    set_decode_type(model, \"greedy\")\n",
        "    costs_list = []\n",
        "    penalties_list = []\n",
        "\n",
        "    for batch in tqdm(dataset.batch(batch_size), disable=disable_tqdm, desc=\"Rollout greedy execution\"):\n",
        "\n",
        "        cost, _, pi = model(batch, return_pi=True)\n",
        "        time_window_penalty = calculate_time_window_penalty(batch, pi)\n",
        "        total_cost = cost + time_window_penalty\n",
        "        costs_list.append(total_cost)\n",
        "        penalties_list.append(time_window_penalty)\n",
        "\n",
        "\n",
        "    return tf.concat(costs_list, axis=0),tf.concat(penalties_list, axis=0)\n",
        "\n",
        "\n",
        "def validate(dataset, model, batch_size=1000):\n",
        "    \"\"\"Validates model on given dataset in greedy mode\n",
        "    \"\"\"\n",
        "    val_costs,val_penalties = rollout(model, dataset, batch_size=batch_size)\n",
        "    set_decode_type(model, \"sampling\")\n",
        "    mean_cost = tf.reduce_mean(val_costs)\n",
        "    mean_penalty = tf.reduce_mean(val_penalties)\n",
        "    print(f\"Validation score: {np.round(mean_cost, 4)}\")\n",
        "    return mean_cost,mean_penalty ## to rewrite\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RolloutBaseline:\n",
        "\n",
        "    def __init__(self, model, filename,\n",
        "                 from_checkpoint=False,\n",
        "                 path_to_checkpoint=None,\n",
        "                 wp_n_epochs=1,\n",
        "                 epoch=0,\n",
        "                 num_samples=10000,\n",
        "                 warmup_exp_beta=0.8,\n",
        "                 embedding_dim=128,\n",
        "                 graph_size=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: current model\n",
        "            filename: suffix for baseline checkpoint filename\n",
        "            from_checkpoint: start from checkpoint flag\n",
        "            path_to_checkpoint: path to baseline model weights\n",
        "            wp_n_epochs: number of warm-up epochs\n",
        "            epoch: current epoch number\n",
        "            num_samples: number of samples to be generated for baseline dataset\n",
        "            warmup_exp_beta: warmup mixing parameter (exp. moving average parameter)\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.cur_epoch = epoch\n",
        "        self.wp_n_epochs = wp_n_epochs\n",
        "        self.beta = warmup_exp_beta\n",
        "\n",
        "        # controls the amount of warmup\n",
        "        self.alpha = 0.0\n",
        "\n",
        "        self.running_average_cost = None\n",
        "\n",
        "        # Checkpoint params\n",
        "        self.filename = filename\n",
        "        self.from_checkpoint = from_checkpoint\n",
        "        self.path_to_checkpoint = path_to_checkpoint\n",
        "\n",
        "        # Problem params\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.graph_size = graph_size\n",
        "\n",
        "        # create and evaluate initial baseline\n",
        "        self._update_baseline(model, epoch)\n",
        "\n",
        "\n",
        "    def _update_baseline(self, model, epoch):\n",
        "\n",
        "        # Load or copy baseline model based on self.from_checkpoint condition\n",
        "        if self.from_checkpoint and self.alpha == 0:\n",
        "            print('Baseline model loaded')\n",
        "            self.model = load_tf_model(self.path_to_checkpoint,\n",
        "                                       embedding_dim=self.embedding_dim,\n",
        "                                       graph_size=self.graph_size)\n",
        "        else:\n",
        "            self.model = copy_of_tf_model(model,\n",
        "                                          embedding_dim=self.embedding_dim,\n",
        "                                          graph_size=self.graph_size)\n",
        "\n",
        "            # For checkpoint\n",
        "            self.model.save_weights('baseline_checkpoint_epoch_{}.h5'.format(self.filename), save_format='h5')\n",
        "\n",
        "        # We generate a new dataset for baseline model on each baseline update to prevent possible overfitting\n",
        "        self.dataset = generate_data_onfly(num_samples=self.num_samples, graph_size=self.graph_size)\n",
        "\n",
        "        print(f\"Evaluating baseline model on baseline dataset (epoch = {epoch})\")\n",
        "        self.bl_vals, self.bl_penalties = rollout(self.model, self.dataset)\n",
        "        self.mean = tf.reduce_mean(self.bl_vals)\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "    def ema_eval(self, cost,penalty):\n",
        "        \"\"\"This is running average of cost through previous batches (only for warm-up epochs)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.running_average_cost is None:\n",
        "            self.running_average_cost = tf.reduce_mean(cost + penalty)\n",
        "        else:\n",
        "            self.running_average_cost = self.beta * self.running_average_cost + (1. - self.beta) * tf.reduce_mean(cost+penalty)\n",
        "\n",
        "        return self.running_average_cost\n",
        "\n",
        "    def eval(self, batch, cost,penalty):\n",
        "        \"\"\"Evaluates current baseline model on single training batch\n",
        "        \"\"\"\n",
        "\n",
        "        if self.alpha == 0:\n",
        "            return self.ema_eval(cost,penalty)\n",
        "\n",
        "        if self.alpha < 1:\n",
        "            v_ema = self.ema_eval(cost,penalty)\n",
        "        else:\n",
        "            v_ema = 0.0\n",
        "\n",
        "\n",
        "        v_b, _, pi = self.model(batch, return_pi=True)\n",
        "        v_b_penalty = calculate_time_window_penalty(batch, pi)\n",
        "        v_b = v_b + v_b_penalty\n",
        "        v_b = tf.stop_gradient(v_b)\n",
        "        v_ema = tf.stop_gradient(v_ema)\n",
        "\n",
        "\n",
        "\n",
        "        # Combination of baseline cost and exp. moving average cost\n",
        "        return self.alpha * v_b + (1 - self.alpha) * v_ema\n",
        "\n",
        "    def eval_all(self, dataset):\n",
        "        \"\"\"Evaluates current baseline model on the whole dataset only for non warm-up epochs\n",
        "        \"\"\"\n",
        "\n",
        "        if self.alpha < 1:\n",
        "            return None\n",
        "\n",
        "        val_costs, val_penalties = rollout(self.model, dataset, batch_size=2048)\n",
        "\n",
        "\n",
        "        return  val_costs\n",
        "\n",
        "    def epoch_callback(self, model, epoch):\n",
        "        \"\"\"Compares current baseline model with the training model and updates baseline if it is improved\n",
        "        \"\"\"\n",
        "\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "        print(f\"Evaluating candidate model on baseline dataset (callback epoch = {self.cur_epoch})\")\n",
        "\n",
        "        candidate_vals, candidate_penalties = rollout(model, self.dataset)  # costs and penalties for training model on baseline dataset\n",
        "\n",
        "        candidate_mean = tf.reduce_mean(candidate_vals)\n",
        "\n",
        "\n",
        "        diff = candidate_mean - self.mean\n",
        "\n",
        "        print(f\"Epoch {self.cur_epoch} candidate mean {candidate_mean}, baseline epoch {self.cur_epoch} mean {self.mean}, difference {diff}\")\n",
        "\n",
        "        if diff < 0:\n",
        "\n",
        "            # statistic + p-value\n",
        "          t, p = ttest_rel(candidate_vals, self.bl_vals)\n",
        "\n",
        "          p_val = p / 2\n",
        "          print(f\"p-value: {p_val}\")\n",
        "\n",
        "          if p_val < 0.05:\n",
        "\n",
        "            print('Update baseline')\n",
        "            self._update_baseline(model, self.cur_epoch)\n",
        "\n",
        "    # alpha controls the amount of warmup\n",
        "        if self.alpha < 1.0:\n",
        "          self.alpha = (self.cur_epoch + 1) / float(self.wp_n_epochs)\n",
        "          print(f\"alpha was updated to {self.alpha}\")\n",
        "\n",
        "\n",
        "def load_tf_model(path, embedding_dim=128, graph_size=6, n_encode_layers=3):\n",
        "    \"\"\"Load model weights from hd5 file\"\"\"\n",
        "    # https://stackoverflow.com/questions/51806852/cant-save-custom-subclassed-model\n",
        "    CAPACITIES = {6: 10., 8: 10.}\n",
        "    TIME_WINDOW_RANGE = 72  # Assuming time windows are within a 24-hour range\n",
        "\n",
        "    depot = tf.random.uniform((2, 2), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    loc = tf.random.uniform((2, graph_size, 2), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    demand = tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(2, graph_size), dtype=tf.int32), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)\n",
        "\n",
        "    # Generate time windows\n",
        "    start_times = tf.random.uniform(minval=0, maxval=TIME_WINDOW_RANGE-24, shape=(2, graph_size), dtype=tf.int32)\n",
        "    end_times = start_times + tf.random.uniform(minval=1, maxval=24, shape=(2, graph_size), dtype=tf.int32)  # Assuming time window duration is between 1 and 6 hours\n",
        "    time_windows = tf.stack([start_times, end_times], axis=-1)\n",
        "\n",
        "    data_random = [depot, loc, demand, time_windows]\n",
        "\n",
        "    model_loaded = AttentionDynamicModel(embedding_dim, n_encode_layers=n_encode_layers)\n",
        "    set_decode_type(model_loaded, \"greedy\")\n",
        "    _, _ = model_loaded(data_random)\n",
        "\n",
        "    model_loaded.load_weights(path)\n",
        "\n",
        "    return model_loaded"
      ],
      "metadata": {
        "id": "RfOl_03_WN7s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Windows Utils"
      ],
      "metadata": {
        "id": "27hBtjl6W17P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_arrival_times(depot, loc, pi):\n",
        "    \"\"\"\n",
        "    Calculate the arrival times at each location in the solution permutation.\n",
        "\n",
        "    Args:\n",
        "        depot (tf.Tensor): Tensor of shape (batch_size, 2) containing the depot coordinates.\n",
        "        loc (tf.Tensor): Tensor of shape (batch_size, n_loc, 2) containing the location coordinates.\n",
        "        pi (tf.Tensor): Tensor of shape (batch_size, n_loc+1) containing the solution permutation.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Tensor of shape (batch_size, n_loc+1) containing the arrival times at each location in the solution permutation.\n",
        "    \"\"\"\n",
        "    batch_size, n_loc, _ = loc.shape\n",
        "\n",
        "    # Create a tensor of coordinates for the depot and locations in the order of the solution permutation\n",
        "    coords = tf.concat([depot[:, None, :], loc], axis=1)  # Shape: (batch_size, n_loc+1, 2)\n",
        "    ordered_coords = tf.gather(coords, tf.cast(pi, tf.int32), batch_dims=1)  # Shape: (batch_size, n_loc+1, 2)\n",
        "\n",
        "    # Calculate the travel times between consecutive locations\n",
        "    travel_times = tf.norm(ordered_coords[:, 1:] - ordered_coords[:, :-1], axis=-1)  # Shape: (batch_size, n_loc)\n",
        "\n",
        "    # Assume a fixed travel speed of 1 unit per time step\n",
        "    travel_speed = 1.0\n",
        "\n",
        "    # Initialize the arrival times with the travel time from the depot to the first location\n",
        "    arrival_times = tf.norm(ordered_coords[:, 1:2] - ordered_coords[:, :1], axis=-1) / travel_speed  # Shape: (batch_size, 1)\n",
        "\n",
        "    # Accumulate the arrival times for the remaining locations\n",
        "    for i in range(1, n_loc):\n",
        "        arrival_times = tf.concat([arrival_times, arrival_times[:, -1:] + travel_times[:, i-1:i] / travel_speed], axis=-1)\n",
        "\n",
        "    return arrival_times\n",
        "def calculate_time_window_penalty(inputs, pi):\n",
        "    \"\"\"\n",
        "    Calculates the time window penalty for a given solution permutation.\n",
        "    Args:\n",
        "        inputs: The input data (depot, loc, demand, time_windows).\n",
        "        pi: The solution permutation.\n",
        "    Returns:\n",
        "        A tensor containing the time window penalty for each instance in the batch.\n",
        "    \"\"\"\n",
        "    depot, loc, _, time_windows = inputs\n",
        "    start_times, end_times = tf.unstack(time_windows, axis=-1)\n",
        "    start_times = tf.cast(start_times, tf.float32)  # Cast start_times to float32\n",
        "    end_times = tf.cast(end_times, tf.float32)  # Cast end_times to float32\n",
        "    arrival_times = calculate_arrival_times(depot, loc, pi)\n",
        "\n",
        "\n",
        "    early_penalties = tf.maximum(0.0, tf.cast(start_times, tf.float32) - arrival_times) * 10\n",
        "    late_penalties = tf.maximum(0.0, arrival_times - tf.cast(end_times, tf.float32)) * 10\n",
        "\n",
        "    time_window_penalty = tf.reduce_sum(early_penalties + late_penalties, axis=-1)\n",
        "\n",
        "    return time_window_penalty"
      ],
      "metadata": {
        "id": "kxN44-PkWNwb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "JlECkPEgW9ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#from attention_dynamic_model import set_decode_type\n",
        "#from reinforce_baseline import validate\n",
        "\n",
        "#from utils import generate_data_onfly, get_results, get_cur_time\n",
        "#from time import gmtime, strftime\n",
        "\n",
        "def train_model(optimizer,\n",
        "                model_tf,\n",
        "                baseline,\n",
        "                validation_dataset,\n",
        "                samples = 1280000,\n",
        "                batch = 128,\n",
        "                val_batch_size = 1000,\n",
        "                start_epoch = 0,\n",
        "                end_epoch = 5,\n",
        "                from_checkpoint = False,\n",
        "                grad_norm_clipping = 1.0,\n",
        "                batch_verbose = 1000,\n",
        "                graph_size = 20,\n",
        "                filename = None\n",
        "                ):\n",
        "\n",
        "    if filename is None:\n",
        "        filename = 'VRP_{}'.format(graph_size)\n",
        "\n",
        "    def rein_loss(model, inputs, baseline, num_batch):\n",
        "\n",
        "\n",
        "        \"\"\"Calculate loss for REINFORCE algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "\n",
        "        cost, log_likelihood, pi = model(inputs, return_pi=True)\n",
        "        time_window_penalty = calculate_time_window_penalty(inputs,pi)\n",
        "        total_cost = cost + time_window_penalty\n",
        "\n",
        "        # Evaluate baseline\n",
        "        # For first wp_n_epochs we take the combination of baseline and ema for previous batches\n",
        "        # after that we take a slice of precomputed baseline values\n",
        "        bl_val = bl_vals[num_batch] if bl_vals is not None else baseline.eval(inputs, cost,time_window_penalty)\n",
        "        bl_val = tf.stop_gradient(bl_val)\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate loss\n",
        "        reinforce_loss = tf.reduce_mean(( total_cost - bl_val) * log_likelihood)\n",
        "\n",
        "        return reinforce_loss, tf.reduce_mean(total_cost)\n",
        "\n",
        "    def grad(model, inputs, baseline, num_batch):\n",
        "        \"\"\"Calculate gradients\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, cost = rein_loss(model, inputs, baseline, num_batch)\n",
        "        return loss, cost, tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # For plotting\n",
        "    train_loss_results = []\n",
        "    train_cost_results = []\n",
        "    val_cost_avg = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "\n",
        "        # Create dataset on current epoch\n",
        "        data = generate_data_onfly(num_samples=samples, graph_size=graph_size)\n",
        "\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        epoch_cost_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        # Skip warm-up stage when we continue training from checkpoint\n",
        "        if from_checkpoint and baseline.alpha != 1.0:\n",
        "            print('Skipping warm-up mode')\n",
        "            baseline.alpha = 1.0\n",
        "\n",
        "        # If epoch > wp_n_epochs then precompute baseline values for the whole dataset else None\n",
        "        bl_vals = baseline.eval_all(data)  # (samples, ) or None\n",
        "        bl_vals = tf.reshape(bl_vals, (-1, batch)) if bl_vals is not None else None # (n_batches, batch) or None\n",
        "\n",
        "        print(\"Current decode type: {}\".format(model_tf.decode_type))\n",
        "\n",
        "        for num_batch, x_batch in tqdm(enumerate(data.batch(batch)), desc=\"batch calculation at epoch {}\".format(epoch)):\n",
        "\n",
        "            # Optimize the model\n",
        "            loss_value, cost_val, grads = grad(model_tf, x_batch, baseline, num_batch)\n",
        "\n",
        "            # Clip gradients by grad_norm_clipping\n",
        "            init_global_norm = tf.linalg.global_norm(grads)\n",
        "            grads, _ = tf.clip_by_global_norm(grads, grad_norm_clipping)\n",
        "            global_norm = tf.linalg.global_norm(grads)\n",
        "\n",
        "            if num_batch%batch_verbose == 0:\n",
        "                print(\"grad_global_norm = {}, clipped_norm = {}\".format(init_global_norm.numpy(), global_norm.numpy()))\n",
        "\n",
        "            optimizer.apply_gradients(zip(grads, model_tf.trainable_variables))\n",
        "\n",
        "            # Track progress\n",
        "            epoch_loss_avg.update_state(loss_value)\n",
        "            epoch_cost_avg.update_state(cost_val)\n",
        "\n",
        "            if num_batch%batch_verbose == 0:\n",
        "                print(\"Epoch {} (batch = {}): Loss: {}: Cost: {}\".format(epoch, num_batch, epoch_loss_avg.result(), epoch_cost_avg.result()))\n",
        "\n",
        "        # Update baseline if the candidate model is good enough. In this case also create new baseline dataset\n",
        "        baseline.epoch_callback(model_tf, epoch)\n",
        "        set_decode_type(model_tf, \"sampling\")\n",
        "\n",
        "        # Save model weights\n",
        "        model_tf.save_weights('model_checkpoint_epoch_{}.h5'.format(filename), save_format='h5')\n",
        "\n",
        "        # Validate current model\n",
        "        val_cost, val_penalty = validate(validation_dataset, model_tf, val_batch_size)\n",
        "        val_cost_avg.append(val_cost)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        train_loss_results.append(epoch_loss_avg.result())\n",
        "\n",
        "        train_cost_results.append(epoch_cost_avg.result())\n",
        "\n",
        "\n",
        "        pd.DataFrame(data={'epochs': list(range(start_epoch, epoch+1)),\n",
        "                           'train_loss': [x.numpy() for x in train_loss_results],\n",
        "                           'train_cost': [x.numpy() for x in train_cost_results],\n",
        "                           'val_cost': [x.numpy() for x in val_cost_avg],\n",
        "\n",
        "                           }).to_csv('backup_results_' + filename + '.csv', index=False)\n",
        "\n",
        "        print(get_cur_time(), \"Epoch {}: Loss: {}: Cost: {}\".format(epoch, epoch_loss_avg.result(), epoch_cost_avg.result()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KeTNEv2fW89x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "QbVhUaiVXE8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "SAMPLES = 512 # 128*10000\n",
        "BATCH = 128\n",
        "FROM_CHECKPOINT = False\n",
        "embedding_dim = 128\n",
        "LEARNING_RATE = 0.00001\n",
        "ROLLOUT_SAMPLES = 10000\n",
        "NUMBER_OF_WP_EPOCHS = 1\n",
        "GRAD_NORM_CLIPPING = 1.0\n",
        "BATCH_VERBOSE = 1000\n",
        "VAL_BATCH_SIZE = 1000\n",
        "VALIDATE_SET_SIZE = 10000\n",
        "SEED = 1234\n",
        "print('Choose your graph size from 6 or 8')\n",
        "GRAPH_SIZE=int(input())\n",
        "FILENAME = 'VRP_{}'.format(GRAPH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoF5zfqiWNs0",
        "outputId": "5f292d21-033c-4404-d32b-39ea41cc297a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose your graph size from 6 or 8\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Epochs\n",
        "START_EPOCH = 0\n",
        "END_EPOCH = 15\n",
        "\n",
        "# Initialize model\n",
        "model_tf = AttentionDynamicModel(embedding_dim)\n",
        "set_decode_type(model_tf, \"sampling\")\n",
        "print('model initialized')\n",
        "\n",
        "# Create and save validation dataset\n",
        "validation_dataset = create_data_on_disk(GRAPH_SIZE,\n",
        "                                         VALIDATE_SET_SIZE,\n",
        "                                         is_save=True,\n",
        "                                         filename='sample',\n",
        "                                         is_return=True,\n",
        "                                         seed = SEED)\n",
        "print('validation dataset created and saved on the disk')\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "# Initialize baseline\n",
        "baseline = RolloutBaseline(model_tf,\n",
        "                           wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
        "                           epoch = 0,\n",
        "                           num_samples=ROLLOUT_SAMPLES,\n",
        "                           filename = FILENAME,\n",
        "                           from_checkpoint = FROM_CHECKPOINT,\n",
        "                           embedding_dim=embedding_dim,\n",
        "                           graph_size=GRAPH_SIZE\n",
        "                           )\n",
        "print('baseline initialized')\n",
        "\n",
        "train_model(optimizer,\n",
        "            model_tf,\n",
        "            baseline,\n",
        "            validation_dataset,\n",
        "            samples = SAMPLES,\n",
        "            batch = BATCH,\n",
        "            val_batch_size = VAL_BATCH_SIZE,\n",
        "            start_epoch = START_EPOCH,\n",
        "            end_epoch = END_EPOCH,\n",
        "            from_checkpoint = FROM_CHECKPOINT,\n",
        "            grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
        "            batch_verbose = BATCH_VERBOSE,\n",
        "            graph_size = GRAPH_SIZE,\n",
        "            filename = FILENAME\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvL20gmTWNpn",
        "outputId": "8fa2e4fd-dc9d-47cf-fd8e-8c15e853eb5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model initialized\n",
            "validation dataset created and saved on the disk\n",
            "Evaluating baseline model on baseline dataset (epoch = 0)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:04<00:00,  2.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline initialized\n",
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rbatch calculation at epoch 0: 0it [00:00, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 505.2741394042969, clipped_norm = 0.9999999403953552\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rbatch calculation at epoch 0: 1it [00:04,  4.34s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 (batch = 0): Loss: 47.976951599121094: Cost: 1339.261474609375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 0: 4it [00:07,  1.95s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 0)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 candidate mean 1341.479248046875, baseline epoch 0 mean 1310.04248046875, difference 31.436767578125\n",
            "alpha was updated to 1.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1340.342041015625\n",
            "2024-06-25 14:18:59 Epoch 0: Loss: -0.29898834228515625: Cost: 1342.106689453125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 1: 1it [00:01,  1.11s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 169.28091430664062, clipped_norm = 1.0\n",
            "Epoch 1 (batch = 0): Loss: -212.525390625: Cost: 1327.9903564453125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 1: 4it [00:04,  1.16s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 1)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 candidate mean 1341.0015869140625, baseline epoch 1 mean 1310.04248046875, difference 30.9591064453125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1339.902099609375\n",
            "2024-06-25 14:19:17 Epoch 1: Loss: -203.82522583007812: Cost: 1313.043701171875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 2: 1it [00:01,  1.12s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 85.45211029052734, clipped_norm = 1.0\n",
            "Epoch 2 (batch = 0): Loss: -216.939453125: Cost: 1324.268310546875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 2: 4it [00:04,  1.11s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 2)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 candidate mean 1341.0791015625, baseline epoch 2 mean 1310.04248046875, difference 31.03662109375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1339.7939453125\n",
            "2024-06-25 14:19:34 Epoch 2: Loss: -199.9245147705078: Cost: 1306.023681640625\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 3: 1it [00:01,  1.22s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 121.54377746582031, clipped_norm = 1.0\n",
            "Epoch 3 (batch = 0): Loss: -233.70095825195312: Cost: 1319.778564453125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 3: 4it [00:04,  1.18s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 3)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 candidate mean 1340.6761474609375, baseline epoch 3 mean 1310.04248046875, difference 30.6336669921875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1339.3941650390625\n",
            "2024-06-25 14:19:52 Epoch 3: Loss: -204.68829345703125: Cost: 1322.736572265625\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 4: 1it [00:01,  1.15s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 130.8418731689453, clipped_norm = 0.9999999403953552\n",
            "Epoch 4 (batch = 0): Loss: -239.79234313964844: Cost: 1317.901611328125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 4: 4it [00:04,  1.12s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 4)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 candidate mean 1340.2271728515625, baseline epoch 4 mean 1310.04248046875, difference 30.1846923828125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1338.9639892578125\n",
            "2024-06-25 14:20:09 Epoch 4: Loss: -211.02444458007812: Cost: 1331.333251953125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 5: 1it [00:01,  1.12s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 114.6907958984375, clipped_norm = 0.9999999403953552\n",
            "Epoch 5 (batch = 0): Loss: -190.50332641601562: Cost: 1334.1468505859375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 5: 4it [00:04,  1.12s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 5)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 candidate mean 1339.92822265625, baseline epoch 5 mean 1310.04248046875, difference 29.8857421875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1338.626220703125\n",
            "2024-06-25 14:20:26 Epoch 5: Loss: -186.77902221679688: Cost: 1322.3104248046875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 6: 1it [00:01,  1.20s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 102.97007751464844, clipped_norm = 0.9999999403953552\n",
            "Epoch 6 (batch = 0): Loss: -159.58251953125: Cost: 1269.5458984375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 6: 4it [00:04,  1.14s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 6)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 candidate mean 1339.8221435546875, baseline epoch 6 mean 1310.04248046875, difference 29.7796630859375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1338.5206298828125\n",
            "2024-06-25 14:20:44 Epoch 6: Loss: -193.2475128173828: Cost: 1307.0618896484375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 7: 1it [00:01,  1.14s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_global_norm = 112.63125610351562, clipped_norm = 0.9999999403953552\n",
            "Epoch 7 (batch = 0): Loss: -170.2759246826172: Cost: 1340.689453125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch calculation at epoch 7: 4it [00:04,  1.15s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 7)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 candidate mean 1339.4803466796875, baseline epoch 7 mean 1310.04248046875, difference 29.4378662109375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation score: 1338.213623046875\n",
            "2024-06-25 14:21:01 Epoch 7: Loss: -207.78900146484375: Cost: 1342.721435546875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 8: 1it [00:01,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 125.49541473388672, clipped_norm = 0.9999999403953552\n",
            "Epoch 8 (batch = 0): Loss: -189.61521911621094: Cost: 1294.748779296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 8: 4it [00:04,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 candidate mean 1339.3709716796875, baseline epoch 8 mean 1310.04248046875, difference 29.3284912109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1338.12939453125\n",
            "2024-06-25 14:21:19 Epoch 8: Loss: -159.30145263671875: Cost: 1310.3311767578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 9: 1it [00:01,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 84.5428695678711, clipped_norm = 1.0\n",
            "Epoch 9 (batch = 0): Loss: -17.24651336669922: Cost: 1346.9349365234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 9: 4it [00:04,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 candidate mean 1339.1424560546875, baseline epoch 9 mean 1310.04248046875, difference 29.0999755859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1337.78466796875\n",
            "2024-06-25 14:21:36 Epoch 9: Loss: -50.84928894042969: Cost: 1325.675537109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 10: 1it [00:01,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 80.9153823852539, clipped_norm = 1.0\n",
            "Epoch 10 (batch = 0): Loss: -147.19281005859375: Cost: 1309.30029296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 10: 4it [00:04,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 candidate mean 1338.86376953125, baseline epoch 10 mean 1310.04248046875, difference 28.8212890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1337.5859375\n",
            "2024-06-25 14:21:53 Epoch 10: Loss: -164.35049438476562: Cost: 1327.914794921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 11: 1it [00:01,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 122.61106872558594, clipped_norm = 0.9999999403953552\n",
            "Epoch 11 (batch = 0): Loss: -133.25765991210938: Cost: 1371.845458984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 11: 4it [00:04,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 candidate mean 1338.483642578125, baseline epoch 11 mean 1310.04248046875, difference 28.441162109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1337.2158203125\n",
            "2024-06-25 14:22:10 Epoch 11: Loss: -149.50039672851562: Cost: 1350.1724853515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 12: 1it [00:01,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 175.7182159423828, clipped_norm = 0.9999999403953552\n",
            "Epoch 12 (batch = 0): Loss: -183.29409790039062: Cost: 1314.2705078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 12: 4it [00:04,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 candidate mean 1338.281982421875, baseline epoch 12 mean 1310.04248046875, difference 28.239501953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1336.86376953125\n",
            "2024-06-25 14:22:28 Epoch 12: Loss: -150.6702423095703: Cost: 1342.334716796875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 13: 1it [00:01,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 104.66818237304688, clipped_norm = 0.9999999403953552\n",
            "Epoch 13 (batch = 0): Loss: -150.80264282226562: Cost: 1307.356201171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 13: 4it [00:04,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 candidate mean 1337.8443603515625, baseline epoch 13 mean 1310.04248046875, difference 27.8018798828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1336.225341796875\n",
            "2024-06-25 14:22:45 Epoch 13: Loss: -126.31867218017578: Cost: 1344.886962890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 14: 1it [00:01,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 178.31118774414062, clipped_norm = 0.9999999403953552\n",
            "Epoch 14 (batch = 0): Loss: -191.8541259765625: Cost: 1324.2652587890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 14: 4it [00:04,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 candidate mean 1337.286376953125, baseline epoch 14 mean 1310.04248046875, difference 27.243896484375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 1335.6639404296875\n",
            "2024-06-25 14:23:02 Epoch 14: Loss: -158.57586669921875: Cost: 1347.1705322265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Inference"
      ],
      "metadata": {
        "id": "U_e6Cvp7Yct3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Choose your graph size from 8 10 20 30')\n",
        "GRAPH_SIZE=int(input())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2D4hvssWNPJ",
        "outputId": "e354d4fb-37fe-4d65-a168-4ddcc36c96bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose your graph size from 8 10 20 30\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate radom dataset. Uncomment this cell if you are planning to generate random data directly\n",
        "\n",
        "VALIDATE_SET_SIZE = 100\n",
        "validation_dataset = create_data_on_disk(GRAPH_SIZE,\n",
        "                                         VALIDATE_SET_SIZE,\n",
        "                                         is_save=True,\n",
        "                                         filename='sample',\n",
        "                                         is_return=True,\n",
        "                                         seed = 121 )\n"
      ],
      "metadata": {
        "id": "k__Kuc13WNIG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_set_path = 'Validation_dataset_sample.pkl'\n",
        "validation_dataset = read_from_pickle(val_set_path)\n",
        "tour = [x for x in validation_dataset.batch(1)][2]\n"
      ],
      "metadata": {
        "id": "3R-FXrTAWNFc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'model_checkpoint_epoch_VRP_6.h5'\n",
        "#Use VRP_50 if the graph size is more than 50\n",
        "#model_path = 'VRP_50.h5'\n",
        "model = load_tf_model(model_path)\n",
        "cost, ll, pi = model(tour, return_pi=True)\n",
        "print('Current cost: ', round(cost.numpy()[0],2))\n",
        "get_journey(tour, pi, GRAPH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tvxo3VkgYn9k",
        "outputId": "7637c80b-84b3-41d9-dd20-5597ba3a02df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current cost:  6.85\n",
            "Current path:  [0.0, 2.0, 3.0, 4.0, 0.0, 6.0, 0.0, 1.0, 0.0, 5.0, 0.0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"cfc3d0da-cea0-4c24-939d-eddd62da1796\" class=\"plotly-graph-div\" style=\"height:1000px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"cfc3d0da-cea0-4c24-939d-eddd62da1796\")) {                    Plotly.newPlot(                        \"cfc3d0da-cea0-4c24-939d-eddd62da1796\",                        [{\"marker\":{\"size\":7},\"mode\":\"markers+text\",\"name\":\"destinations\",\"opacity\":1.0,\"text\":[\"(0, 3.0)\",\"(1, 1.0)\",\"(2, 4.0)\",\"(3, 4.0)\",\"(4, 8.0)\",\"(5, 9.0)\"],\"textposition\":\"top center\",\"x\":[0.37502872943878174,0.7483206987380981,0.7508783340454102,0.48233044147491455,0.21607017517089844,0.8543717861175537],\"y\":[0.27734851837158203,0.15613436698913574,0.5614955425262451,0.819608211517334,0.34767138957977295,0.32949769496917725],\"type\":\"scatter\"},{\"marker\":{\"size\":15},\"mode\":\"markers+text\",\"name\":\"depot\",\"text\":[\"1.0\"],\"textposition\":\"bottom center\",\"x\":[0.05184650421142578],\"y\":[0.9154417514801025],\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"path_1, length=2.25\",\"opacity\":1.0,\"x\":[0.05184650421142578,0.7483206987380981,0.7508783340454102,0.48233044147491455,0.05184650421142578],\"y\":[0.9154417514801025,0.15613436698913574,0.5614955425262451,0.819608211517334,0.9154417514801025],\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"path_2, length=1.99\",\"opacity\":1.0,\"x\":[0.05184650421142578,0.8543717861175537,0.05184650421142578],\"y\":[0.9154417514801025,0.32949769496917725,0.9154417514801025],\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"path_3, length=1.43\",\"opacity\":1.0,\"x\":[0.05184650421142578,0.37502872943878174,0.05184650421142578],\"y\":[0.9154417514801025,0.27734851837158203,0.9154417514801025],\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"path_4, length=1.18\",\"opacity\":1.0,\"x\":[0.05184650421142578,0.21607017517089844,0.05184650421142578],\"y\":[0.9154417514801025,0.34767138957977295,0.9154417514801025],\"type\":\"scatter\"}],                        {\"height\":1000,\"showlegend\":true,\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"\\u003cb\\u003e6_Customers_ML_Model_Total_Distance_6.85\\u003c\\u002fb\\u003e\"},\"width\":1000,\"xaxis\":{\"title\":{\"text\":\"X coordinate\"}},\"yaxis\":{\"title\":{\"text\":\"Y coordinate\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cfc3d0da-cea0-4c24-939d-eddd62da1796');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HN-X4aKrkPAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}